%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%     Chapter 4
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Conclusion \& Future Work}\label{conclusion}

\section{Conclusion}
We started the dissertation by introducing statistical machine translation and we gave a brief overview about the steps involved in training a phrase-based statistical machine translation system using a parallel corpus. When decoding a source sentence to translate it into a target language, we show that the decoder has very little information about source words outside the current phrase pair in consideration. Little work has been done in the literature that try to incorporate information about source words outside the current phrase pair in consideration. In the quest of providing the decoder more information from words in the source sentence, \cite{Niehues2011} introduced bilingual language models. They achieved statistically significant gains by replacing words with part-of-speech tags and then creating their bilingual language models. \cite{Stewart2014} extended their work and showed that significant gains can be achieved by using a combination of coarse language models and coarse bilingual language models. \cite{Stewart2014} made their language models coarse by clustering their corpora using \textbf{mkcls}, a popular monolingual word clustering algorithm. Their approach depends on using word alignments and monolingual clusters to create the bitokens. This approach will not be able to capture the information provided by words which are not direct translations of each other as captured by word alignments. In order to include information from words which are not direct translations of each other, we proposed a novel approach of using word embeddings and bilingual word embeddings to create coarse language models and bilingual language models.

In this dissertation we present three new systems of using word embeddings and bilingual word embeddings to create coarse language models and bilingual language models. In all three systems we create bilingual word embeddings using BiCVM~\cite{Hermann14} and cluster these embeddings using \textbf{greedo}~\cite{Stratos2014}. The clusters are used to augment the Chinese-English parallel corpus. These augmented corporas are used to create coarse language models in all three of our systems. In two our systems we use augmented corporas to create bitokens, whereas in the third system we use the Chinese-English parallel corpus to create the bitokens.  In the first two systems, we create coarse bilingual language models using the bitokens itself. In all three systems, we further cluster the bitokens. We experiment with clustering the bitokens using \textbf{mkcls} and also by creating bitoken embeddings using \textbf{word2vec}. The bitoken embeddings are clustered again using \textbf{greedo}. The clusters are then used to augment the bitokens and this augmented bitoken corpus is used to create coarse bilingual language models.

In our experiments we compare our systems to a baseline system, which is an implementation of \cite{Stewart2014}. We show that all three of our systems outperform the baseline system. When looking at the BLEU score, the second system which uses coarse bilingual language models by creating bitoken embeddings performs the best and when looking at TER score, the first system which uses \textbf{mkcls} to cluster the bitokens performs the best. The second system has a $p$-value of 0.00 when looking at BLEU score, that is the improvements are statistically significant, hence we deem it as the winning system out of all three of our systems. Overall, we achieve an improvement of 1.4 BLEU points compared to our baseline system.

\section{Future Work}
\subsection{Clustering of Embeddings}
In our systems, we cluster the embeddings using \textbf{greedo}~\cite{Stratos2014}. \textbf{greedo} creates a hierarchical cluster of the embeddings by measuring the euclidean distances. As \textbf{greedo} and \textbf{mkcls} are based on \cite{Brown1992} model, it made it easier to compare our systems to the baseline system. 

Word embeddings show unique properties when we measure their similarity using cosine similarity. Word embeddings which are have a high cosine similarity tend to be semantically similar. Based on this idea, we would like to experiment with clustering algorithms that use cosine similarity as their distance measure. Specifically, we would like to experiment with using spherical k-means clustering~\cite{Hornik2012} as it uses cosine similarity as its distance measure.

\subsection{Extending Bi-LMs to Translation Model}
Even though, Bi-LMs are language models, but they act more as translation models as they do not model the fluency of target language but model the translation of source words. Based on this idea, we would like to extend the idea of using word embeddings in translation model. In phrase-based SMT, the translation model consists of phrase pairs. One way to modify the translation model to include embeddings would be to have a translation model that contains phrase embedding pairs instead of words in phrases. We would like to test this new embeddings based translation model as a standalone translation model and also as an additional translation model that complements the standard word based translation model.